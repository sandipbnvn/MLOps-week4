name: MLOps Pipeline CI/CD

on:
  pull_request:
    branches: [ main, stg ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run unit tests
      run: |
        cd mlops-pipeline
        python -m pytest tests/ -v
    
    - name: Run integration tests
      run: |
        cd mlops-pipeline
        python -m pytest tests/test_integration.py -v
    
    - name: Run complete pipeline
      run: |
        cd mlops-pipeline
        python run_pipeline.py

  ml-pipeline:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r mlops-pipeline/requirements.txt
    
    - name: Run MLOps pipeline
      run: |
        cd mlops-pipeline
        python run_pipeline.py
    
    - name: Setup CML
      uses: iterative/setup-cml@v1
    
    - name: Generate and post accuracy report
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        cd mlops-pipeline
        
        # Load metrics from the pipeline output
        if [ -f "artifacts/metrics.json" ]; then
          echo "## ðŸ“Š Model Performance Report" >> report.md
          echo "" >> report.md
          
          # Extract accuracy score using Python
          python -c '
import json
import sys

try:
    with open("artifacts/metrics.json", "r") as f:
        data = json.load(f)
    
    metrics = data.get("metrics", {})
    accuracy = metrics.get("accuracy", "N/A")
    
    if isinstance(accuracy, (int, float)):
        print(f"**Accuracy Score:** {accuracy:.4f}")
    else:
        print(f"**Accuracy Score:** {accuracy}")
    
except Exception as e:
    print(f"**Error loading metrics:** {str(e)}")
    sys.exit(1)
' >> report.md
        
        else
          echo "## âŒ Pipeline Failed" >> report.md
          echo "" >> report.md
          echo "The MLOps pipeline failed to complete successfully." >> report.md
          echo "Please check the logs for more details." >> report.md
        fi
        
        # Post the report as a comment on the PR
        cml comment create report.md --pr-id ${{ github.event.pull_request.number }}
    
    - name: Upload artifacts
      uses: actions/upload-artifact@v3
      with:
        name: ml-artifacts
        path: |
          mlops-pipeline/artifacts/
          mlops-pipeline/coverage.xml
          mlops-pipeline/htmlcov/
        retention-days: 30

  quality-check:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r mlops-pipeline/requirements.txt
    
    - name: Run code formatting check
      run: |
        cd mlops-pipeline
        black --check src/ tests/ run_pipeline.py
    
    - name: Run linting
      run: |
        cd mlops-pipeline
        flake8 src/ tests/ run_pipeline.py --max-line-length=88 --ignore=E203,W503 